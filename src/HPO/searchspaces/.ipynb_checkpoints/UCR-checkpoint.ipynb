{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2bf28ff-748a-456e-a827-d476756182de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import HPO\n",
    "from HPO.data.UEA.download import UEA_Handler\n",
    "import numpy as np \n",
    "import time\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from HPO.utils.model import NetworkMain\n",
    "from HPO.utils.DARTS_utils import config_space_2_DARTS\n",
    "from HPO.utils.FCN import FCN \n",
    "import pandas as pd\n",
    "import torch\n",
    "from HPO.data.teps_datasets import Train_TEPS , Test_TEPS\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import random\n",
    "import HPO.utils.augmentation as aug\n",
    "from HPO.utils.train_utils import collate_fn_padd\n",
    "from HPO.utils.train import train_model, train_model_triplet\n",
    "from HPO.utils.weight_freezing import freeze_FCN, freeze_resnet\n",
    "from HPO.utils.ResNet1d import resnet18\n",
    "from HPO.utils.files import save_obj\n",
    "from queue import Empty\n",
    "from sklearn.model_selection import StratifiedKFold as KFold\n",
    "from collections import namedtuple\n",
    "from HPO.utils.worker_score import Evaluator \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from HPO.utils.worker_utils import LivePlot\n",
    "import seaborn as sns\n",
    "Genotype = namedtuple('Genotype', 'normal normal_concat reduce reduce_concat')\n",
    "model = 0\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "856dfa8c-7d0c-4dbc-88eb-2ddb3968b481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Metadata from /home/cmackinnon/scripts/datasets/UEA/UEA_meta.csv\n"
     ]
    }
   ],
   "source": [
    "datasets = UEA_Handler(\"/home/cmackinnon/scripts/datasets/UEA/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8faa04fc-3a30-454b-9bec-51d2c2f5b184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArticularyWordRecognition',\n",
       " 'AtrialFibrillation',\n",
       " 'BasicMotions',\n",
       " 'CharacterTrajectories',\n",
       " 'Cricket',\n",
       " 'DuckDuckGeese',\n",
       " 'EigenWorms',\n",
       " 'Epilepsy',\n",
       " 'EthanolConcentration',\n",
       " 'ERing',\n",
       " 'FaceDetection',\n",
       " 'FingerMovements',\n",
       " 'HandMovementDirection',\n",
       " 'Handwriting',\n",
       " 'Heartbeat',\n",
       " 'InsectWingbeat',\n",
       " 'JapaneseVowels',\n",
       " 'Libras',\n",
       " 'LSST',\n",
       " 'MotorImagery',\n",
       " 'NATOPS',\n",
       " 'PenDigits',\n",
       " 'PEMS-SF',\n",
       " 'Phoneme',\n",
       " 'RacketSports',\n",
       " 'SelfRegulationSCP1',\n",
       " 'SelfRegulationSCP2',\n",
       " 'SpokenArabicDigits',\n",
       " 'StandWalkJump',\n",
       " 'UWaveGestureLibrary']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdb98560-be25-4dc6-b400-82193cada1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(hyperparameter,dataset_train = None,  dataset_test = None,trainloader = None, cuda_device = None, binary = False, evaluator= None):\n",
    "  ### Configuration \n",
    "  THRESHOLD = 0.4 #Cut off for classification\n",
    "  batch_size = 8\n",
    "  if cuda_device == None:\n",
    "     cuda_device = 1# torch.cuda.current_device()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "  if dataset_train == None:\n",
    "      ##Set up augmentations##\n",
    "      jitter = aug.Jitter(device = cuda_device,sigma = 0.125, rate = 0.5)\n",
    "      crop = aug.Crop(device = cuda_device, rate = 0.8, crop_min = 0.3 , crop_max = 0.98)\n",
    "      scaling = aug.Scaling(device = cuda_device)\n",
    "      window_warp = aug.WindowWarp(device = cuda_device,rate = 0.5)\n",
    "      cut_out = aug.CutOut(device = cuda_device)\n",
    "      mix_up = aug.MixUp(device = cuda_device,m = 0.2, rate = 0.3)\n",
    "      augmentations = [jitter,crop,scaling, window_warp, cut_out]\n",
    "      dataset_train = Train_TEPS(augmentations= augmentations, samples_per_class = 500,device = cuda_device,one_hot = False,binary = False)\n",
    "\n",
    "  #dataset_test_full = Test_TEPS()\n",
    "  torch.cuda.set_device(cuda_device)\n",
    "\n",
    "  print(\"Cuda Device Value: \", cuda_device)\n",
    "  gen = config_space_2_DARTS(hyperparameter,reduction = True)\n",
    "  print(gen)\n",
    "  print(\"channels: {}\".format(hyperparameter[\"channels\"]))\n",
    "\n",
    "  n_classes = dataset_train.get_n_classes()\n",
    "  multibatch = False\n",
    "  torch.cuda.empty_cache()\n",
    "  if trainloader == None:\n",
    "      trainloader = torch.utils.data.DataLoader(\n",
    "                          dataset_train,collate_fn = collate_fn_padd,shuffle = True,\n",
    "                          batch_size=batch_size, drop_last = True)\n",
    "\n",
    "\n",
    "  model = NetworkMain(dataset_train.get_n_features(),hyperparameter[\"channels\"],num_classes= dataset_train.n_classes, \n",
    "                      layers = hyperparameter[\"layers\"], auxiliary = False,drop_prob = hyperparameter[\"p\"], genotype = gen, binary = binary)\n",
    "  model = model.cuda(device = cuda_device)\n",
    "  \"\"\"\n",
    "  ### Train the model\n",
    "  \"\"\"\n",
    "  train_model(model , hyperparameter, trainloader , hyperparameter[\"epochs\"], batch_size , cuda_device, binary = binary,evaluator = evaluator) \n",
    "  torch.cuda.empty_cache()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8edd2e1-6494-4bc4-9bd6-235f09f2a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute(hyperparameter,dataset_train = None,  dataset_test = None,trainloader = None, cuda_device = None, binary = False,evaluator = None):\n",
    "  ### Configuration \n",
    "  THRESHOLD = 0.4 #Cut off for classification\n",
    "  batch_size = 8\n",
    "  if cuda_device == None:\n",
    "     cuda_device = 1# torch.cuda.current_device()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "  if dataset_train == None:\n",
    "      ##Set up augmentations##\n",
    "      jitter = aug.Jitter(device = cuda_device,sigma = 0.125, rate = 0.5)\n",
    "      crop = aug.Crop(device = cuda_device, rate = 0.8, crop_min = 0.3 , crop_max = 0.98)\n",
    "      scaling = aug.Scaling(device = cuda_device)\n",
    "      window_warp = aug.WindowWarp(device = cuda_device,rate = 0.5)\n",
    "      cut_out = aug.CutOut(device = cuda_device)\n",
    "      mix_up = aug.MixUp(device = cuda_device,m = 0.2, rate = 0.3)\n",
    "      augmentations = [jitter,crop,scaling, window_warp, cut_out]\n",
    "      dataset_train = Train_TEPS(augmentations= augmentations, samples_per_class = 500,device = cuda_device,one_hot = False,binary = False)\n",
    "\n",
    "  #dataset_test_full = Test_TEPS()\n",
    "  torch.cuda.set_device(cuda_device)\n",
    "\n",
    "  print(\"Cuda Device Value: \", cuda_device)\n",
    "  gen = config_space_2_DARTS(hyperparameter,reduction = True)\n",
    "  print(gen)\n",
    "  print(\"channels: {}\".format(hyperparameters[\"channels\"]))\n",
    "\n",
    "\n",
    "  n_classes = dataset_train.get_n_classes()\n",
    "  multibatch = False\n",
    "  torch.cuda.empty_cache()\n",
    "  if trainloader == None:\n",
    "      trainloader = torch.utils.data.DataLoader(\n",
    "                          dataset_train,collate_fn = collate_fn_padd,shuffle = True,\n",
    "                          batch_size=batch_size, drop_last = True)\n",
    "\n",
    "\n",
    "  model = NetworkMain(dataset_train.get_n_features(),hyperparameter[\"channels\"],num_classes= 3, \n",
    "                      layers = hyperparameter[\"layers\"], auxiliary = False,drop_prob = hyperparameter[\"p\"], genotype = gen, binary = binary)\n",
    "  model = model.cuda(device = cuda_device)\n",
    "  \"\"\"\n",
    "  ### Train the model\n",
    "  \"\"\"\n",
    "  train_model_triplet(model , hyperparameter, trainloader , hyperparameter[\"epochs\"], batch_size , cuda_device, binary = binary,evaluator = evaluator) \n",
    "  torch.cuda.empty_cache()\n",
    "  for i, (sample, label) in enumerate( trainloader):\n",
    "    embedded = model(sample.float()).cpu().detach().numpy()\n",
    "    if i == 0:\n",
    "        train_embedded = embedded\n",
    "    else:\n",
    "        train_embedded = np.concatenate((train_embedded,embedded),axis = 0)\n",
    "    if i == 0:\n",
    "        train_labels = label.cpu().detach().numpy()\n",
    "    else:\n",
    "        train_labels = np.concatenate((train_labels,label.cpu().detach().numpy()),axis = 0)   \n",
    "        \n",
    "  for angle in range(0, 360,60):\n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        for i in np.unique(train_labels):\n",
    "            idx = np.where(train_labels ==i)\n",
    "            ax.scatter(train_embedded[idx,0],train_embedded[idx,1],train_embedded[idx,2],label = i)\n",
    "        plt.legend()\n",
    "        ax.view_init(angle, angle)\n",
    "        plt.draw()\n",
    "        \n",
    "  knn = KNeighborsClassifier(5)\n",
    "  knn = knn.fit( train_embedded,train_labels)\n",
    "  print(knn.score( train_embedded,train_labels))\n",
    "  return model, knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55a44174-eea9-4f1c-a810-9af8b0d0e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HPO.searchspaces.AttnNAS_config import init_config\n",
    "cs = init_config()\n",
    "from HPO.searchspaces.DARTS_new_config import init_config as init_config_hpo\n",
    "cs_hpo = init_config_hpo()\n",
    "results_df = pd.DataFrame(columns = [\"labels\",\"prediction\",\"risk\",\"augmented\",\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3725e8e0-8ded-4891-9cf8-2b23a97aa1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_hpo.sample_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e3c8326-ee51-4b55-a6a1-f048f578aa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration(values={\n",
       "  'channels': 4,\n",
       "  'layers': 6,\n",
       "  'normal_index_0_0': 1,\n",
       "  'normal_index_0_1': 0,\n",
       "  'normal_index_1_0': 2,\n",
       "  'normal_index_1_1': 2,\n",
       "  'normal_index_2_0': 2,\n",
       "  'normal_index_2_1': 2,\n",
       "  'normal_index_3_0': 0,\n",
       "  'normal_index_3_1': 0,\n",
       "  'normal_index_4_0': 5,\n",
       "  'normal_index_4_1': 4,\n",
       "  'normal_index_5_0': 0,\n",
       "  'normal_index_5_1': 4,\n",
       "  'normal_index_6_0': 7,\n",
       "  'normal_index_6_1': 0,\n",
       "  'normal_index_7_0': 2,\n",
       "  'normal_index_7_1': 5,\n",
       "  'normal_node_0_0': 'attention_channel',\n",
       "  'normal_node_0_1': 'SE_8',\n",
       "  'normal_node_1_0': 'depth_conv_201',\n",
       "  'normal_node_1_1': 'SE_16',\n",
       "  'normal_node_2_0': 'depth_conv_61',\n",
       "  'normal_node_2_1': 'none',\n",
       "  'normal_node_3_0': 'max_pool_3x3',\n",
       "  'normal_node_3_1': 'max_pool_3x3',\n",
       "  'normal_node_4_0': 'attention_space',\n",
       "  'normal_node_4_1': 'depth_conv_61',\n",
       "  'normal_node_5_0': 'attention_channel',\n",
       "  'normal_node_5_1': 'depth_conv_7',\n",
       "  'normal_node_6_0': 'depth_conv_15',\n",
       "  'normal_node_6_1': 'avg_pool_31x31',\n",
       "  'normal_node_7_0': 'max_pool_3x3',\n",
       "  'normal_node_7_1': 'depth_conv_7',\n",
       "  'reduction_index_0_0': 1,\n",
       "  'reduction_index_0_1': 0,\n",
       "  'reduction_index_1_0': 0,\n",
       "  'reduction_index_1_1': 2,\n",
       "  'reduction_index_2_0': 0,\n",
       "  'reduction_index_2_1': 1,\n",
       "  'reduction_index_3_0': 3,\n",
       "  'reduction_index_3_1': 2,\n",
       "  'reduction_index_4_0': 3,\n",
       "  'reduction_index_4_1': 5,\n",
       "  'reduction_index_5_0': 6,\n",
       "  'reduction_index_5_1': 2,\n",
       "  'reduction_index_6_0': 5,\n",
       "  'reduction_index_6_1': 7,\n",
       "  'reduction_index_7_0': 5,\n",
       "  'reduction_index_7_1': 7,\n",
       "  'reduction_node_0_0': 'depth_conv_201',\n",
       "  'reduction_node_0_1': 'SE_16',\n",
       "  'reduction_node_1_0': 'max_pool_31x31',\n",
       "  'reduction_node_1_1': 'depth_conv_101',\n",
       "  'reduction_node_2_0': 'skip_connect',\n",
       "  'reduction_node_2_1': 'skip_connect',\n",
       "  'reduction_node_3_0': 'depth_conv_7',\n",
       "  'reduction_node_3_1': 'attention_space',\n",
       "  'reduction_node_4_0': 'point_conv',\n",
       "  'reduction_node_4_1': 'max_pool_31x31',\n",
       "  'reduction_node_5_0': 'depth_conv_29',\n",
       "  'reduction_node_5_1': 'depth_conv_15',\n",
       "  'reduction_node_6_0': 'avg_pool_64x64',\n",
       "  'reduction_node_6_1': 'avg_pool_3x3',\n",
       "  'reduction_node_7_0': 'depth_conv_101',\n",
       "  'reduction_node_7_1': 'depth_conv_201',\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.sample_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7cd1ac3-321b-4902-bd44-6d6d5fb516b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.list_datasets()\n",
    "ds.remove('DuckDuckGeese')\n",
    "ds.remove(\"FaceDetection\")\n",
    "ds.remove(\"InsectWingbeat\")\n",
    "ds = [\"FingerMovements\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95096f71-d540-4eba-a89e-453ef7f18fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f29d1070-9c38-457f-b523-f89f0c8d9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdff37cc-68a8-450f-8621-a97405630d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(Problem='ArticularyWordRecognition', TrainSize='275', TestSize='300', NumDimensions='9', SeriesLength='144', NumClasses='25', Normalised='true', Padded='false', MissingValues='false', ClassCounts=['11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '11', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12'])\n",
      "Dataset(Problem='AtrialFibrillation', TrainSize='15', TestSize='15', NumDimensions='2', SeriesLength='640', NumClasses='3', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['5', '5', '5', '5', '5'])\n",
      "Dataset(Problem='BasicMotions', TrainSize='40', TestSize='40', NumDimensions='6', SeriesLength='100', NumClasses='4', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['10', '10', '10', '10', '10', '10', '10'])\n",
      "Dataset(Problem='CharacterTrajectories', TrainSize='1422', TestSize='1436', NumDimensions='3', SeriesLength='182', NumClasses='20', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['70', '71', '78', '93', '69', '63', '87', '62', '65', '70', '65', '62', '59', '66', '65', '77', '62', '68', '85', '86', '71', '71', '79', '93', '69', '64', '87', '63', '65', '71', '66', '62', '60', '67', '66', '78', '63', '69', '86'])\n",
      "Dataset(Problem='Cricket', TrainSize='108', TestSize='72', NumDimensions='6', SeriesLength='1197', NumClasses='12', Normalised='true', Padded='false', MissingValues='false', ClassCounts=['9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6'])\n",
      "Dataset(Problem='DuckDuckGeese', TrainSize='50', TestSize='50', NumDimensions='1345', SeriesLength='270', NumClasses='5', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['10', '10', '10', '10', '10', '10', '10', '10', '10'])\n",
      "Dataset(Problem='EigenWorms', TrainSize='128', TestSize='131', NumDimensions='6', SeriesLength='17984', NumClasses='5', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['22', '17', '22', '12', '55', '22', '18', '23', '13'])\n",
      "Dataset(Problem='Epilepsy', TrainSize='137', TestSize='138', NumDimensions='3', SeriesLength='206', NumClasses='4', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['37', '36', '30', '34', '37', '37', '30'])\n",
      "Dataset(Problem='EthanolConcentration', TrainSize='261', TestSize='263', NumDimensions='3', SeriesLength='1751', NumClasses='4', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['65', '66', '65', '66', '66', '66', '65'])\n",
      "Dataset(Problem='ERing', TrainSize='30', TestSize='270', NumDimensions='4', SeriesLength='65', NumClasses='6', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['5', '5', '5', '5', '5', '45', '45', '45', '45', '45', '45'])\n",
      "Dataset(Problem='FaceDetection', TrainSize='5890', TestSize='3524', NumDimensions='144', SeriesLength='62', NumClasses='2', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['2945', '1762', '1762'])\n",
      "Dataset(Problem='FingerMovements', TrainSize='316', TestSize='100', NumDimensions='28', SeriesLength='50', NumClasses='2', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['157', '49', '51'])\n",
      "Dataset(Problem='HandMovementDirection', TrainSize='160', TestSize='74', NumDimensions='10', SeriesLength='400', NumClasses='4', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['40', '40', '40', '14', '30', '15', '15'])\n",
      "Dataset(Problem='Handwriting', TrainSize='150', TestSize='850', NumDimensions='3', SeriesLength='152', NumClasses='26', Normalised='true', Padded='false', MissingValues='false', ClassCounts=['6', '9', '4', '5', '9', '6', '2', '3', '7', '2', '5', '7', '4', '8', '5', '6', '6', '3', '5', '5', '10', '4', '4', '6', '11', '36', '34', '29', '43', '34', '30', '29', '32', '27', '26', '25', '37', '40', '35', '29', '33', '32', '31', '33', '31', '34', '43', '32', '37', '26', '32'])\n",
      "Dataset(Problem='Heartbeat', TrainSize='204', TestSize='205', NumDimensions='61', SeriesLength='405', NumClasses='2', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['147', '57', '148'])\n",
      "Dataset(Problem='InsectWingbeat', TrainSize='30000', TestSize='20000', NumDimensions='200', SeriesLength='30', NumClasses='10', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['3000', '3000', '3000', '3000', '3000', '3000', '3000', '3000', '3000', '2000', '2000', '2000', '2000', '2000', '2000', '2000', '2000', '2000', '2000'])\n",
      "Dataset(Problem='JapaneseVowels', TrainSize='270', TestSize='370', NumDimensions='12', SeriesLength='29', NumClasses='9', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['30', '30', '30', '30', '30', '30', '30', '30', '31', '35', '88', '44', '29', '24', '40', '50', '29'])\n",
      "Dataset(Problem='Libras', TrainSize='180', TestSize='180', NumDimensions='2', SeriesLength='45', NumClasses='15', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12'])\n",
      "Dataset(Problem='LSST', TrainSize='2459', TestSize='2466', NumDimensions='6', SeriesLength='36', NumClasses='14', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['123', '270', '381', '62', '7', '153', '23', '313', '68', '120', '777', '77', '51', '35', '124', '270', '382', '63', '7', '153', '24', '313', '68', '121', '777', '77', '52'])\n",
      "Dataset(Problem='MotorImagery', TrainSize='278', TestSize='100', NumDimensions='64', SeriesLength='3000', NumClasses='2', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['139', '50', '50'])\n",
      "Dataset(Problem='NATOPS', TrainSize='180', TestSize='180', NumDimensions='24', SeriesLength='51', NumClasses='6', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['30', '30', '30', '30', '30', '30', '30', '30', '30', '30', '30'])\n",
      "Dataset(Problem='PenDigits', TrainSize='7494', TestSize='3498', NumDimensions='2', SeriesLength='8', NumClasses='10', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['779', '780', '719', '780', '720', '720', '778', '719', '719', '363', '364', '364', '336', '364', '335', '336', '364', '336', '336'])\n",
      "Dataset(Problem='PEMS-SF', TrainSize='267', TestSize='173', NumDimensions='963', SeriesLength='144', NumClasses='7', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['32', '38', '42', '43', '36', '44', '30', '25', '26', '23', '22', '27', '20'])\n",
      "Dataset(Problem='Phoneme', TrainSize='3315', TestSize='3353', NumDimensions='11', SeriesLength='217', NumClasses='39', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '85', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '86', '85'])\n",
      "Dataset(Problem='RacketSports', TrainSize='151', TestSize='152', NumDimensions='6', SeriesLength='30', NumClasses='4', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['43', '35', '34', '40', '43', '35', '34'])\n",
      "Dataset(Problem='SelfRegulationSCP1', TrainSize='268', TestSize='293', NumDimensions='6', SeriesLength='896', NumClasses='2', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['133', '147', '146'])\n",
      "Dataset(Problem='SelfRegulationSCP2', TrainSize='200', TestSize='180', NumDimensions='7', SeriesLength='1152', NumClasses='2', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['100', '90', '90'])\n",
      "Dataset(Problem='SpokenArabicDigits', TrainSize='6599', TestSize='2199', NumDimensions='13', SeriesLength='93', NumClasses='10', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['660', '660', '660', '660', '660', '660', '660', '660', '659', '220', '220', '220', '220', '220', '220', '220', '220', '220', '219'])\n",
      "Dataset(Problem='StandWalkJump', TrainSize='12', TestSize='15', NumDimensions='4', SeriesLength='2500', NumClasses='3', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['4', '4', '5', '5', '5'])\n",
      "Dataset(Problem='UWaveGestureLibrary', TrainSize='120', TestSize='320', NumDimensions='3', SeriesLength='315', NumClasses='8', Normalised='true', Padded='false', MissingValues='false', ClassCounts=['15', '15', '15', '15', '15', '15', '15', '40', '40', '40', '40', '40', '40', '40', '40'])\n"
     ]
    }
   ],
   "source": [
    "for name in datasets.datasets:\n",
    "    print(datasets.datasets[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a244e1f-44af-4f78-ab6f-3b2defed6f41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: FingerMovements Shape: (28, 316)\n",
      "Name: FingerMovements Shape: (28, 100)\n",
      "Loading: Dataset(Problem='FingerMovements', TrainSize='316', TestSize='100', NumDimensions='28', SeriesLength='50', NumClasses='2', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['157', '49', '51']) \n",
      "(100, 28, 50)\n",
      "[b'left', b'right']\n",
      "Loading: Dataset(Problem='FingerMovements', TrainSize='316', TestSize='100', NumDimensions='28', SeriesLength='50', NumClasses='2', Normalised='false', Padded='false', MissingValues='false', ClassCounts=['157', '49', '51']) \n",
      "(316, 28, 50)\n",
      "[b'left', b'right']\n",
      "Model is binary:False\n",
      "{'channels': 3, 'layers': 5, 'normal_index_0_0': 1, 'normal_index_0_1': 1, 'normal_index_1_0': 0, 'normal_index_1_1': 0, 'normal_index_2_0': 3, 'normal_index_2_1': 3, 'normal_index_3_0': 2, 'normal_index_3_1': 3, 'normal_index_4_0': 0, 'normal_index_4_1': 5, 'normal_index_5_0': 1, 'normal_index_5_1': 1, 'normal_index_6_0': 6, 'normal_index_6_1': 4, 'normal_index_7_0': 8, 'normal_index_7_1': 7, 'normal_node_0_0': 'depth_conv_61', 'normal_node_0_1': 'depth_conv_15', 'normal_node_1_0': 'SE_16', 'normal_node_1_1': 'depth_conv_15', 'normal_node_2_0': 'depth_conv_15', 'normal_node_2_1': 'point_conv', 'normal_node_3_0': 'avg_pool_3x3', 'normal_node_3_1': 'SE_8', 'normal_node_4_0': 'SE_8', 'normal_node_4_1': 'max_pool_64x64', 'normal_node_5_0': 'depth_conv_29', 'normal_node_5_1': 'SE_8', 'normal_node_6_0': 'none', 'normal_node_6_1': 'avg_pool_3x3', 'normal_node_7_0': 'depth_conv_29', 'normal_node_7_1': 'depth_conv_101', 'reduction_index_0_0': 1, 'reduction_index_0_1': 0, 'reduction_index_1_0': 0, 'reduction_index_1_1': 0, 'reduction_index_2_0': 1, 'reduction_index_2_1': 1, 'reduction_index_3_0': 2, 'reduction_index_3_1': 2, 'reduction_index_4_0': 4, 'reduction_index_4_1': 1, 'reduction_index_5_0': 3, 'reduction_index_5_1': 4, 'reduction_index_6_0': 1, 'reduction_index_6_1': 2, 'reduction_index_7_0': 8, 'reduction_index_7_1': 2, 'reduction_node_0_0': 'attention_channel', 'reduction_node_0_1': 'max_pool_64x64', 'reduction_node_1_0': 'SE_16', 'reduction_node_1_1': 'depth_conv_29', 'reduction_node_2_0': 'SE_8', 'reduction_node_2_1': 'attention_channel', 'reduction_node_3_0': 'depth_conv_101', 'reduction_node_3_1': 'max_pool_64x64', 'reduction_node_4_0': 'depth_conv_201', 'reduction_node_4_1': 'avg_pool_3x3', 'reduction_node_5_0': 'depth_conv_15', 'reduction_node_5_1': 'none', 'reduction_node_6_0': 'max_pool_3x3', 'reduction_node_6_1': 'none', 'reduction_node_7_0': 'depth_conv_61', 'reduction_node_7_1': 'depth_conv_101'}\n",
      "Cuda Device Value:  1\n",
      "Genotype(normal=[('depth_conv_61', 1), ('depth_conv_15', 1), ('SE_16', 0), ('depth_conv_15', 0), ('depth_conv_15', 3), ('point_conv', 3), ('avg_pool_3x3', 2), ('SE_8', 3), ('SE_8', 0), ('max_pool_64x64', 5), ('depth_conv_29', 1), ('SE_8', 1), ('none', 6), ('avg_pool_3x3', 4), ('depth_conv_29', 8), ('depth_conv_101', 7)], normal_concat=[9], reduce=[('attention_channel', 1), ('max_pool_64x64', 0), ('SE_16', 0), ('depth_conv_29', 0), ('SE_8', 1), ('attention_channel', 1), ('depth_conv_101', 2), ('max_pool_64x64', 2), ('depth_conv_201', 4), ('avg_pool_3x3', 1), ('depth_conv_15', 3), ('none', 4), ('max_pool_3x3', 1), ('none', 2), ('depth_conv_61', 8), ('depth_conv_101', 2)], reduce_concat=[5, 6, 7, 9])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hyperparameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6425/898832618.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mcuda_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6425/2458629873.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(hyperparameter, dataset_train, dataset_test, trainloader, cuda_device, binary, evaluator)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_space_2_DARTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"channels: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"channels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hyperparameters' is not defined"
     ]
    }
   ],
   "source": [
    "binary = False\n",
    "for name in ds:\n",
    "    cuda_device = 1\n",
    "    jitter = aug.Jitter(device = cuda_device,sigma = 0.0125, rate = 0.25)\n",
    "    crop = aug.Crop(device = cuda_device, rate = 0.5, crop_min = 0.3 , crop_max = 0.98)\n",
    "    window_warp = aug.WindowWarp(device = cuda_device,rate = 0.25)\n",
    "    augmentations = [jitter,crop, window_warp]\n",
    "    train_args = [False, 1 ,None,1]\n",
    "    test_args = [False, 1 , None,1]\n",
    "    train_dataset, test_dataset = datasets.load_all(name,train_args,test_args)\n",
    "    binary = False#True if train_dataset.n_classes == 2 else False\n",
    "    print(\"Model is binary:{}\".format(binary))\n",
    "    hyperparameter = cs.sample_configuration().get_dictionary()\n",
    "    print(hyperparameter)\n",
    "    hpo = {'channels': 2**hyperparameter[\"channels\"], 'lr': 0.0025170869707739693, 'p': 0.00, 'epochs': 10}\n",
    "    hyperparameter.update(hpo)\n",
    "    #dataset_test = Test_TEPS(binary = False,samples_per_class = 16, one_hot = False)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      test_dataset,collate_fn = collate_fn_padd,shuffle = True,\n",
    "                      batch_size=8,drop_last = True)\n",
    "\n",
    "    n_classes = test_dataset.get_n_classes()\n",
    "    evaluator = Evaluator(8, test_dataset.get_n_classes(),cuda_device,testloader = testloader) \n",
    "    \n",
    "    \n",
    "    model = compute(hyperparameter,dataset_train = train_dataset,binary = binary)\n",
    "    cuda_device = 1\n",
    " \n",
    "    THRESHOLD = 0.4\n",
    "    #batch_size = 2\n",
    "    model.eval()\n",
    "\n",
    "    evaluator.forward_pass(model, testloader,binary)\n",
    "    evaluator.predictions(model_is_binary = binary , THRESHOLD = THRESHOLD)\n",
    "    total = evaluator.T()\n",
    "    acc  =  evaluator.T_ACC()\n",
    "    recall = evaluator.TPR(1)\n",
    "    recall_total = evaluator.P(1)\n",
    "    print(\"Accuracy: \", \"%.4f\" % ((acc)*100), \"%\")\n",
    "    print(\"Recall: \", \"%.4f\" % ((recall)*100), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd187e9-9d03-48cb-8ae5-49881253c6a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in ds:\n",
    "    cuda_device = 1\n",
    "    jitter = aug.Jitter(device = cuda_device,sigma = 0.0125, rate = 0.25)\n",
    "    crop = aug.Crop(device = cuda_device, rate = 0.25, crop_min = 0.3 , crop_max = 0.98)\n",
    "    window_warp = aug.WindowWarp(device = cuda_device,rate = 0.25)\n",
    "    augmentations = [jitter,crop, window_warp]\n",
    "    train_args = [False, 1 ,None,1]\n",
    "    test_args = [False, 1 , None,1]\n",
    "    train_dataset, test_dataset = datasets.load_all(name,train_args,test_args)\n",
    "\n",
    "    hyperparameter = cs.sample_configuration().get_dictionary()\n",
    "    print(hyperparameter)\n",
    "    hpo = {'channels': 32, 'lr': 0.0025170869707739693, 'p': 0.00, 'epochs': 10}\n",
    "    hyperparameter.update(hpo)\n",
    "    model,knn = _compute(hyperparameter,dataset_train = train_dataset)\n",
    "    cuda_device = 1\n",
    "    binary = False\n",
    "    THRESHOLD = 0.4\n",
    "    batch_size = 2\n",
    "    model.eval()\n",
    "    #dataset_test = Test_TEPS(binary = False,samples_per_class = 16, one_hot = False)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      test_dataset,collate_fn = collate_fn_padd,shuffle = True,\n",
    "                      batch_size=batch_size,drop_last = True)\n",
    "    \n",
    "    for i, (sample, label) in enumerate( testloader):\n",
    "        embedded = model(sample.float()).cpu().detach().numpy()\n",
    "\n",
    "        if i == 0:\n",
    "            test_embedded = embedded\n",
    "        else:\n",
    "            test_embedded = np.concatenate((test_embedded,embedded),axis = 0)\n",
    "        if i == 0:\n",
    "            test_labels = label.cpu().detach().numpy()\n",
    "        else:\n",
    "            test_labels = np.concatenate((test_labels,label.cpu().detach().numpy()),axis = 0)  \n",
    "    print(\"Test Score: {}%\".format(knn.score( test_embedded, test_labels )))\n",
    "    pred = knn.predict(test_embedded)\n",
    "    with np.printoptions(linewidth = (10*21),precision=4, suppress=True):\n",
    "        print(confusion_matrix(pred,test_labels))    \n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        for i in np.unique(test_labels):\n",
    "            idx = np.where(test_labels ==i)\n",
    "            ax.scatter(test_embedded[idx,0],test_embedded[idx,1],test_embedded[idx,2],label = i)\n",
    "        plt.legend()\n",
    "        plt.title(\"Testing Data\")\n",
    "        plt.show()\n",
    "        \n",
    "    \"\"\"\n",
    "    n_classes = test_dataset.get_n_classes()\n",
    "    evaluator = Evaluator(batch_size, test_dataset.get_n_classes(),cuda_device,testloader = testloader) \n",
    "    evaluator.forward_pass(model, testloader,binary)\n",
    "    evaluator.predictions(model_is_binary = binary , THRESHOLD = THRESHOLD)\n",
    "    total = evaluator.T()\n",
    "    acc  =  evaluator.T_ACC()\n",
    "    recall = evaluator.TPR(1)\n",
    "    recall_total = evaluator.P(1)\n",
    "    print(\"Accuracy: \", \"%.4f\" % ((acc)*100), \"%\")\n",
    "    print(\"Recall: \", \"%.4f\" % ((recall)*100), \"%\")\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8cb988-9163-4c1b-a347-b9d2cd135934",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in ds:\n",
    "    batch_size = 16\n",
    "    cuda_device = 1\n",
    "    jitter = aug.Jitter(device = cuda_device,sigma = 0.0125, rate = 0.25)\n",
    "    crop = aug.Crop(device = cuda_device, rate = 0.25, crop_min = 0.3 , crop_max = 0.98)\n",
    "    window_warp = aug.WindowWarp(device = cuda_device,rate = 0.25)\n",
    "    augmentations = [jitter,crop, window_warp]\n",
    "    train_args = [False, 1 , None ,1]\n",
    "    test_args = [False, 1 , None,1]\n",
    "    dataset = datasets.load_mixed(name,train_args,test_args)\n",
    "    kfold = KFold(n_splits = 5, shuffle = True)\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset,y = list(dataset.y.cpu().numpy())*1)):\n",
    "        print(\"Size of Train: {} Size of Test : {} -- Fold No. {}\".format(len(train_ids),len(test_ids),fold))\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                        dataset, collate_fn = collate_fn_padd,\n",
    "                        batch_size=batch_size, sampler=train_subsampler,drop_last = True)\n",
    "\n",
    "\n",
    "        hyperparameter = cs.sample_configuration().get_dictionary()\n",
    "        hyperparameter = {'normal_index_0_0': 0, 'normal_index_0_1': 1, 'normal_index_1_0': 2, 'normal_index_1_1': 1, 'normal_index_2_0': 3, 'normal_index_2_1': 2, \n",
    "                      'normal_index_3_0': 2, 'normal_index_3_1': 4, 'normal_node_0_0': 'dil_conv_3x3', 'normal_node_0_1': 'none', 'normal_node_1_0': 'sep_conv_5x5', \n",
    "                      'normal_node_1_1': 'max_pool_3x3', 'normal_node_2_0': 'avg_pool_3x3', 'normal_node_2_1': 'sep_conv_7x7', 'normal_node_3_0': 'skip_connect', \n",
    "                      'normal_node_3_1': 'sep_conv_7x7', 'reduction_index_0_0': 0, 'reduction_index_0_1': 0, 'reduction_index_1_0': 1, 'reduction_index_1_1': 1, \n",
    "                      'reduction_index_2_0': 1, 'reduction_index_2_1': 3, 'reduction_index_3_0': 2, 'reduction_index_3_1': 2, 'reduction_node_0_0': 'none', \n",
    "                      'reduction_node_0_1': 'skip_connect', 'reduction_node_1_0': 'sep_conv_7x7', 'reduction_node_1_1': 'sep_conv_5x5', 'reduction_node_2_0': 'dil_conv_5x5', \n",
    "                      'reduction_node_2_1': 'skip_connect', 'reduction_node_3_0': 'skip_connect', 'reduction_node_3_1': 'sep_conv_5x5'    }\n",
    "        hpo = {'channels': 32, 'lr': 0.0025170869707739693, 'p': 0.00, 'epochs': 50, 'layers': 3}\n",
    "        hyperparameter.update(hpo)\n",
    "        model,knn = _compute(hyperparameter,trainloader = trainloader ,dataset_train = dataset)\n",
    "        cuda_device = 1\n",
    "        binary = False\n",
    "        THRESHOLD = 0.4\n",
    "        batch_size = 16\n",
    "        dataset.augmentation = False\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "                dataset, collate_fn = collate_fn_padd,\n",
    "                batch_size=batch_size, sampler=test_subsampler,drop_last = True)\n",
    "        model.eval()\n",
    "        #dataset_test = Test_TEPS(binary = False,samples_per_class = 16, one_hot = False)\n",
    "        for i, (sample, label) in enumerate( testloader):\n",
    "            embedded = model(sample.float()).cpu().detach().numpy()\n",
    "\n",
    "            if i == 0:\n",
    "                test_embedded = embedded\n",
    "            else:\n",
    "                test_embedded = np.concatenate((test_embedded,embedded),axis = 0)\n",
    "            if i == 0:\n",
    "                test_labels = label.cpu().detach().numpy()\n",
    "            else:\n",
    "                test_labels = np.concatenate((test_labels,label.cpu().detach().numpy()),axis = 0)  \n",
    "        print(\"Test Score: {}%\".format(knn.score( test_embedded, test_labels )))\n",
    "        pred = knn.predict(test_embedded)\n",
    "        with np.printoptions(linewidth = (10*21),precision=4, suppress=True):\n",
    "            print(confusion_matrix(pred,test_labels))    \n",
    "            fig = plt.figure(figsize=(12, 12))\n",
    "            ax = fig.add_subplot(projection='3d')\n",
    "            for i in np.unique(test_labels):\n",
    "                idx = np.where(test_labels ==i)\n",
    "                ax.scatter(test_embedded[idx,0],test_embedded[idx,1],test_embedded[idx,2],label = i)\n",
    "            plt.legend()\n",
    "            plt.title(\"Testing Data\")\n",
    "            plt.show()\n",
    "\n",
    "        \"\"\"\n",
    "        n_classes = test_dataset.get_n_classes()\n",
    "        evaluator = Evaluator(batch_size, test_dataset.get_n_classes(),cuda_device,testloader = testloader) \n",
    "        evaluator.forward_pass(model, testloader,binary)\n",
    "        evaluator.predictions(model_is_binary = binary , THRESHOLD = THRESHOLD)\n",
    "        total = evaluator.T()\n",
    "        acc  =  evaluator.T_ACC()\n",
    "        recall = evaluator.TPR(1)\n",
    "        recall_total = evaluator.P(1)\n",
    "        print(\"Accuracy: \", \"%.4f\" % ((acc)*100), \"%\")\n",
    "        print(\"Recall: \", \"%.4f\" % ((recall)*100), \"%\")\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324890a1-481a-4e31-a671-1e2a80db3905",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((50,1)) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ad597-32cc-40b3-a0b1-e1e1368b3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = datasets.load_all('Cricket',train_args,test_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae8764-a3b6-42f0-84dc-83273e28e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dataset))\n",
    "x = x.cpu().numpy()\n",
    "for i in x:\n",
    "    plt.plot(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933bce36-081c-4662-8fa0-bb8b2cbf7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7928ac-a80f-45a9-9080-6ba01b4ea9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
